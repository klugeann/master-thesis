#!python3

import openai
import random
import time
import os
import pandas as pd
import math
from statsmodels.stats.power import TTestIndPower


def read_lines_from_text_file(file_name):
    """
    Read lines from a text file and return a list of non-empty lines.

    Args:
        file_name (str): The name of the text file to read.

    Returns:
        list: A list of non-empty lines from the text file.
    """
    # Make file name relative to the script's location
    file_name = os.path.join(os.path.dirname(__file__), file_name)

    with open(file_name, "r") as open_file:
        stripped_lines = [line.strip() for line in open_file]
        return [line for line in stripped_lines if line]


def read_text_from_file(file_name):
    """
    Read the contents of a text file and return them as a single string.

    Args:
        file_name (str): The name of the text file to read.

    Returns:
        str: The contents of the text file as a single string.
    """
    lines = read_lines_from_text_file(file_name)
    return " ".join(lines)


def read_dictionary_from_file(file_name):
    """
    Reads a dictionary from a text file.

    Args:
        file_name (str): The name of the text file.

    Returns:
        dict: The dictionary read from the file.

    File Format:
        The text file should have one key-value pair per line, separated by a colon (:).
        Example:
        key1:value1
        key2:value2
        ...
    """
    def make_pair(line):
        pair = [el.strip() for el in line.split(":", 2)]
        return ["", line] if len(pair) < 2 else pair

    lines = read_lines_from_text_file(file_name)
    list_of_pairs = [make_pair(line) for line in lines]

    # Combine list of pairs into a dictionry of keys and multiple values
    dictionary = {}
    for key, value in list_of_pairs:
        if key not in dictionary:
            dictionary[key] = [value]
        else:
            dictionary[key].append(value)
    return dictionary
    

def ask_llm(pre_prompt, prompt, config):
    """
    Sends a prompt to the OpenAI Chat API and returns the response.

    Parameters:
    prompt (str): The user's prompt.

    Returns:
    str: The response generated by the OpenAI Chat API.
    """
    
    def query_openai(config):

        client = openai.OpenAI(
            api_key = config.get("api_key"),
            base_url = config.get("base_url")
        )

        completion = client.chat.completions.create(
            model=config.get("model"),
            messages=[
                {"role": "system", "content": pre_prompt},
                {"role": "user", "content": prompt}
            ]
        )

        return completion.choices[0].message.content
    
    return query_openai(config or {})


def find_sample_size(effect_size, alpha, power, num_groups):
    """
    Calculate the sample size needed for a given effect size, alpha, power, and number of groups.

    Parameters:
    effect_size (float): The effect size.
    alpha (float): The significance level.
    power (float): The desired power.
    num_groups (int): The number of groups.

    Returns:
    int: The sample size needed for each group (rounded up).
    """
    power_analysis = TTestIndPower()
    sample_size = power_analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power, nobs1=None, ratio=1.0, alternative="two-sided")

    return math.ceil(sample_size)


if __name__ == "__main__":
    # Query directory from user
    directory = input("Enter the directory containing the survey files: ")

    # Query whether to use causal inference or not
    is_causal_inference = input("Do you want to use causal inference? (y/N): ").lower() == "y"

    # Read general instruction pre-prompt for LLM
    if is_causal_inference:
        pre_prompt = read_text_from_file(f"{directory}/prompt_causal_inference.txt")
        
        # Read CSV with causal inference data
        causal_inference_data = pd.read_csv(f"{directory}/causal_inference_data.csv")

        # Inject causal inference data into the pre-prompt
        pre_prompt = pre_prompt.format(
            # TODO: Add causal inference data to the pre-prompt
        )

    else:
        pre_prompt = read_text_from_file(f"{directory}/prompt.txt")

    # Read the three experimental scenarios
    scenarios = read_dictionary_from_file(f"{directory}/scenarios.txt")

    # Read survey questions (formatted for LLM)
    survey_questions = read_dictionary_from_file(f"{directory}/questions.txt")
    survey_question_categories = sorted(list(survey_questions.keys()))

    # Query if dry running
    dry_run = input("Do you want to do a dry run? (y,N): ").lower() == "y"

    # Query OpenAI API config from user
    llm_config = {}

    if dry_run:
        print("Running in dry run mode. No API key provided")
    else:
        api_key = input("Enter your OpenAI API key (or empty): ")

        if api_key:
            llm_config["api_key"] = api_key

        base_url = input("Enter your OpenAI base URL (or empty): ")
    
        if base_url:
            llm_config["base_url"] = base_url
        
        model = input("Enter your model: ")

        if model:
            llm_config["model"] = model
        

    # Query output file name from user
    output_file = input("Enter the name of the output file: ")

    # Find number of trials using the sample size calculation
    num_trials = find_sample_size(
        effect_size=0.4,           # Medium effect size based on Hufnagel et al. (2022)
        alpha=0.05,                # Standard significance level of 0.05
        power=0.80,                # Desired statistical power of 0.80
        num_groups=len(scenarios)  # Number of experimental scenarios
    )

    print(f"Sample size calculation: {num_trials} trials needed for the survey.")

    # Create empty array for dataset
    rows = []

    # Run trials (adjust as needed)
    for trial_id in range(num_trials):

        # Randomly select a scenario
        selected_scenario = random.choice(list(scenarios.keys()))
        scenario_text = scenarios[selected_scenario][0]

        print(f"{trial_id + 1}/{num_trials} Running LLM survey for: {selected_scenario}")

        question_id = 1

        # Query the LLM for each survey question
        for category in survey_question_categories:
            for question in survey_questions[category]:
                full_prompt = f"{scenario_text}\n\n{question}"

                # If no API key provided, skip API call
                if dry_run:
                    llm_response = "[DRY RUN]"
                else:
                    llm_response = ask_llm(pre_prompt, full_prompt, llm_config)

                # Question id as a 3 digit number with the question
                question_column = f"{question_id:03d} {question}"
                question_id += 1

                # Store the survey response in the DataFrame
                rows.append({
                    "TrialId": trial_id,
                    "Scenario": selected_scenario,
                    "Category": category,
                    "Question": question_column,
                    "Response": llm_response
                })

            # Avoid hitting API rate limits
            if not dry_run:
                # time.sleep(1)
                pass # Deactivate

    # Convert the list of rows to a DataFrame
    df = pd.DataFrame(rows, columns=["TrialId", "Scenario", "Category", "Question", "Response"])

    # Put questions as columns
    df = df.pivot_table(index=["TrialId", "Scenario"], columns=["Question", "Category"], values="Response", aggfunc="first").reset_index()

    # Save the survey responses to a CSV file with columns as trials and rows as questions
    df.to_csv(output_file, index=False)

    print(f"LLM survey responses saved to '{output_file}'.")
